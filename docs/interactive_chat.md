# Интерактивный чат с Ollama

Этот документ описывает использование интерактивного чата для общения с системой мониторинга.

## Что это?

`chat_with_ollama.py` - это интерактивный терминальный чат, который позволяет:

- Задавать вопросы о состоянии сервера на естественном языке
- Получать метрики CPU, памяти, дисков
- Просматривать ошибки в логах
- Получать анализ и рекомендации от LLM (Ollama)

## Запуск

```powershell
# Перейти в директорию MCP сервера
cd monitoring-poc/mcp-server

# Активировать виртуальное окружение
.\venv\Scripts\Activate.ps1

# Запустить чат
python chat_with_ollama.py
```

## Доступные команды

### Быстрые команды

| Команда | Описание |
|---------|----------|
| `/cpu` | Показать текущую загрузку CPU |
| `/memory` | Показать использование памяти |
| `/disk` | Показать использование дисков |
| `/logs` | Показать последние ошибки в логах |
| `/status` | Полный статус системы |
| `/help` | Показать справку |
| `/exit` | Выход из чата |

### Примеры быстрых команд

```
Вы> /cpu
CPU: 45.23% (порог: 80%) - НОРМАЛЬНАЯ

Вы> /memory
Memory:
  Total:     15.63 GB
  Used:      8.45 GB
  Available: 7.18 GB
  Usage:     54.05% (порог: 85%)
  Status:    НОРМАЛЬНОЕ

Вы> /status
СТАТУС СИСТЕМЫ (2025-01-15 14:30:25):

CPU: 45.23% (порог: 80%) - НОРМАЛЬНАЯ
...
```

## Вопросы на естественном языке

Вы можете задавать любые вопросы о системе, и LLM (Ollama) ответит с учетом текущих метрик.

### Примеры вопросов

**О состоянии системы:**
```
Вы> Какая сейчас нагрузка на сервере?
Вы> Всё ли в порядке с сервером?
Вы> Есть ли какие-то проблемы?
```

**О ресурсах:**
```
Вы> Много ли занято памяти?
Вы> Хватает ли места на дисках?
Вы> CPU сильно загружен?
```

**Об ошибках:**
```
Вы> Есть ли ошибки в логах?
Вы> Что случилось с сервером?
Вы> Какие проблемы были за последний час?
```

**Рекомендации:**
```
Вы> Что мне делать с высокой нагрузкой CPU?
Вы> Нужно ли добавлять память?
Вы> Как оптимизировать работу сервера?
```

## Как это работает

1. **Вы задаете вопрос** - на русском языке, простыми словами
2. **Система собирает метрики** - CPU, память, диски, логи с удаленного сервера
3. **Данные отправляются в Ollama** - локальную LLM с контекстом о системе
4. **LLM анализирует и отвечает** - с учетом реальных метрик и пороговых значений

## Примеры использования

### Пример 1: Проверка состояния

```
Вы> Всё ли в порядке с сервером?

[~] Получение метрик...
[~] Отправка запроса в Ollama (может занять 30-60 сек)...

На сервере всё стабильно:
- CPU загружен на 45%, это нормальный уровень
- Память используется на 54%, есть запас
- Диски заполнены в пределах нормы
- Ошибок за последний час не обнаружено

Рекомендация: Система работает штатно, вмешательства не требуется.
```

### Пример 2: Анализ нагрузки

```
Вы> CPU сильно загружен?

[~] Получение метрик...
[~] Отправка запроса в Ollama (может занять 30-60 сек)...

Текущая загрузка CPU составляет 45.23%, что значительно ниже
критического порога в 80%. Это нормальный уровень нагрузки для
рабочего сервера. Резерв производительности составляет около 35%.
```

### Пример 3: Поиск проблем

```
Вы> Есть ли ошибки в логах?

[~] Получение метрик...
[~] Отправка запроса в Ollama (может занять 30-60 сек)...

За последний час обнаружено 3 ошибки:
1. Docker контейнер monitoring-poc_prometheus - ошибка сети
2. Promtail - таймаут подключения к Loki
3. Node Exporter - временная недоступность метрики

Все ошибки носят временный характер и были исправлены
автоматически. Текущее состояние: все сервисы работают.
```

## Советы по использованию

### 1. Формулируйте вопросы четко

✅ **Хорошо:**
- "Какая загрузка CPU?"
- "Есть ли ошибки за последний час?"
- "Сколько свободной памяти?"

❌ **Плохо:**
- "Ну как там?"
- "А что?"
- "Проблемы?"

### 2. Будьте конкретны

Если хотите узнать о чем-то конкретном, спрашивайте прямо:

```
Вы> Какая загрузка CPU на сервере сейчас?
```

Лучше, чем:
```
Вы> Расскажи про сервер
```

### 3. Используйте команды для быстрых проверок

Для частых запросов используйте команды:
- `/status` - быстрый обзор всей системы
- `/cpu` - только CPU
- `/logs` - только ошибки

Это быстрее, чем задавать вопрос через LLM.

### 4. LLM может занять время

Первый запрос к Ollama после запуска может занять 30-60 секунд.
Это нормально - модель загружается в память.

Последующие запросы будут быстрее (10-20 секунд).

## Troubleshooting

### "Ollama: [FAIL]"

**Проблема:** Ollama не запущен или недоступен

**Решение:**
```powershell
# Проверить, запущен ли Ollama
ollama list

# Если не запущен, запустить
ollama serve

# В другом окне терминала запустить чат
python chat_with_ollama.py
```

### "Prometheus: [FAIL]"

**Проблема:** Удаленный сервер недоступен

**Решение:**
1. Проверьте интернет-соединение
2. Проверьте доступность сервера: `http://147.45.157.2:9090`
3. Проверьте настройки в `.env`

### Долгий ответ от LLM

**Проблема:** Ollama долго думает (более 60 секунд)

**Причины:**
- Первый запуск модели (загрузка в память)
- Большой объем данных для анализа
- Слабое железо компьютера

**Решение:**
- Подождите завершения первого запроса
- Используйте более легкую модель (`llama3` вместо `mistral`)
- Увеличьте `OLLAMA_TIMEOUT` в `.env`

### Ошибка "UnicodeEncodeError"

**Проблема:** Windows терминал не поддерживает UTF-8

**Решение:**
```powershell
# В PowerShell
[Console]::OutputEncoding = [System.Text.Encoding]::UTF8

# Затем запустить чат
python chat_with_ollama.py
```

## Интеграция в дипломную работу

Этот чат - отличная демонстрация для диплома:

### Что показать:

1. **Запуск чата** - показать интерфейс
2. **Быстрые команды** - `/status`, `/cpu`, `/memory`
3. **Вопросы на естественном языке** - "Есть ли проблемы?"
4. **Анализ LLM** - показать, как LLM анализирует метрики
5. **Рекомендации** - как система дает советы

### Сценарий демонстрации:

```
1. Запуск: python chat_with_ollama.py
2. Проверка: /status
3. Вопрос: "Всё ли в порядке с сервером?"
4. Детали: /cpu, /memory, /logs
5. Анализ: "Что делать с высокой нагрузкой?"
```

## Дальнейшее развитие

Возможные улучшения:

- [ ] Веб-интерфейс вместо терминала
- [ ] История диалога с сохранением
- [ ] Графики метрик прямо в чате
- [ ] Автоматические уведомления при проблемах
- [ ] Интеграция с Telegram/Slack
- [ ] Голосовые команды

