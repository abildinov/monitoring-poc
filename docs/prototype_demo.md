# MCP Server Prototype - Демонстрация

## Обзор

Рабочий прототип MCP-сервера для мониторинга серверной инфраструктуры с интеграцией локальной LLM (Ollama).

**Дата создания**: 10 октября 2025  
**Версия**: 1.0.0 (Working Prototype)  
**Автор**: Абильдинов Алексей

## Что реализовано

### ✅ Инфраструктура

- **Виртуальное окружение**: Python venv в `mcp-server/`
- **Зависимости**: Все установлены (MCP SDK, httpx, pydantic, loguru, pandas, numpy)
- **Конфигурация**: `.env` файл с настройками

### ✅ HTTP Клиенты

1. **PrometheusClient** (`clients/prometheus_client.py`)
   - Подключение к удаленному Prometheus (147.45.157.2:9090)
   - Методы: `get_current_cpu()`, `get_current_memory()`, `get_disk_usage()`
   - Health check и обработка ошибок

2. **LokiClient** (`clients/loki_client.py`)
   - Подключение к удаленному Loki (147.45.157.2:3100)
   - Методы: `get_error_logs()`, `search_logs()`, `get_logs_by_container()`
   - LogQL запросы с фильтрацией

3. **OllamaClient** (`llm/ollama_client.py`)
   - Подключение к локальной Ollama (localhost:11434)
   - Модель: llama3 (8B параметров)
   - Методы: `analyze_metrics()`, `analyze_logs()`, `detect_anomaly()`
   - Стриминг поддержка

### ✅ MCP Server

**Файл**: `server.py`

**3 реализованных MCP Tools**:

1. **get_cpu_usage**
   - Получает текущую загрузку CPU с сервера
   - Анализирует через LLM
   - Возвращает значение, threshold, status, и рекомендации

2. **get_memory_status**
   - Получает состояние RAM (total, used, available, percent)
   - Анализирует через LLM
   - Возвращает детальную информацию и рекомендации

3. **search_error_logs**
   - Ищет ошибки в логах за указанный период (по умолчанию 1 час)
   - Получает до 20 ошибок из Loki
   - Анализирует через LLM
   - Возвращает список ошибок и анализ

### ✅ Тестирование

**Файл**: `test_server.py`

Локальное тестирование всех tools без Claude Desktop.

**Результаты последнего теста**:

```
TEST 1: GET_CPU_USAGE
  CPU: 7.97%
  Threshold: 80%
  Status: NORMAL
  [OK]

TEST 2: GET_MEMORY_STATUS
  Total: 3.82 GB
  Used: 0.86 GB (22.46%)
  Threshold: 85%
  Status: NORMAL
  [OK]

TEST 3: SEARCH_ERROR_LOGS
  Период: 24 часа
  Найдено ошибок: 0
  [OK]

ИТОГО: ВСЕ ТЕСТЫ ПРОЙДЕНЫ ✓
```

## Архитектура

```
┌─────────────────────────────────────┐
│  Удаленный Сервер                   │
│  147.45.157.2                       │
│  ├─ Prometheus :9090                │
│  ├─ Loki :3100                      │
│  └─ Grafana :3000                   │
└─────────────────────────────────────┘
            ↕ HTTP
┌─────────────────────────────────────┐
│  Локальный Компьютер                │
│  ┌─────────────────────────────┐    │
│  │  MCP Server (server.py)     │    │
│  │  ├─ PrometheusClient         │    │
│  │  ├─ LokiClient               │    │
│  │  └─ 3 MCP Tools              │    │
│  └─────────────────────────────┘    │
│            ↕                        │
│  ┌─────────────────────────────┐    │
│  │  Ollama (localhost:11434)   │    │
│  │  Model: llama3              │    │
│  └─────────────────────────────┘    │
│            ↕                        │
│  ┌─────────────────────────────┐    │
│  │  Claude Desktop (будущее)   │    │
│  └─────────────────────────────┘    │
└─────────────────────────────────────┘
```

## Использование

### Локальное тестирование

```powershell
cd monitoring-poc/mcp-server

# Активировать venv
.\venv\Scripts\Activate.ps1

# Запустить тесты
python test_server.py
```

### Запуск MCP сервера

```powershell
# Запуск для Claude Desktop
python server.py
```

## Технические детали

### Конфигурация

**Файл**: `.env`

```env
# Удаленные сервисы
PROMETHEUS_URL=http://147.45.157.2:9090
LOKI_URL=http://147.45.157.2:3100

# Локальная LLM
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3
OLLAMA_TIMEOUT=120

# Пороги для анализа
CPU_THRESHOLD=80
MEMORY_THRESHOLD=85
DISK_THRESHOLD=90
```

### Зависимости

**Основные**:
- `mcp>=0.9.0` - MCP SDK
- `httpx>=0.27.0` - HTTP клиент
- `pydantic>=2.5.0` - Валидация конфигурации
- `loguru>=0.7.0` - Логирование

**Опциональные**:
- `pandas>=2.1.0` - Обработка данных
- `numpy>=1.24.0` - Математика
- `pytest>=7.4.0` - Тестирование

### Производительность

**Время выполнения tools** (на реальных данных):

- `get_cpu_usage`: ~2-3 минуты (из-за Ollama генерации)
- `get_memory_status`: ~2-3 минуты
- `search_error_logs`: ~5-10 секунд (если нет ошибок)

**Узкие места**:
- Ollama генерация занимает 100-120 секунд (timeout)
- Можно оптимизировать: уменьшить температуру, использовать более быструю модель, кэшировать промпты

## Известные ограничения

1. **Ollama timeout**: Генерация LLM может занимать 120+ секунд
   - *Решение*: Увеличить `OLLAMA_TIMEOUT` или использовать более быструю модель

2. **Кодировка Windows**: Эмодзи не отображаются в консоли
   - *Решение*: Заменены на ASCII символы `[+]`, `[-]`, `[~]`

3. **Grafana UI не работает**: API работает, но веб-интерфейс имеет проблемы
   - *Решение*: Используем только API, UI не критичен

4. **Нет кэширования**: Каждый запрос идет в LLM
   - *Решение*: Добавить Redis или файловое кэширование (следующая версия)

## Следующие шаги

### Краткосрочные (1-2 недели)

- [ ] Интеграция с Claude Desktop
- [ ] Добавить 7-10 дополнительных tools:
  - `get_disk_usage`
  - `get_network_io`
  - `get_container_stats`
  - `get_system_summary`
  - `analyze_timerange`
  - `get_alert_history`
  - `check_thresholds`

- [ ] Улучшить промпты для Ollama
- [ ] Добавить кэширование результатов

### Среднесрочные (1 месяц)

- [ ] Базовая аналитика (Z-score, percentiles)
- [ ] Baseline calculator
- [ ] Детекция аномалий
- [ ] Unit-тесты
- [ ] CI/CD pipeline

### Долгосрочные (2-3 месяца)

- [ ] REST API (FastAPI)
- [ ] Web интерфейс
- [ ] Система алертинга
- [ ] ML-модели для прогнозирования
- [ ] Поддержка multiple LLM провайдеров

## Демонстрация для диплома

### Кейс 1: Анализ CPU

**Запрос**: "Какая загрузка CPU на сервере?"

**Результат**:
```
CPU Usage: 7.97%
Threshold: 80%
Status: NORMAL

Анализ LLM:
Загрузка CPU находится на низком уровне (7.97%), что 
значительно ниже порогового значения 80%. Система работает 
стабильно, ресурсы используются эффективно. Рекомендаций 
по оптимизации не требуется.
```

### Кейс 2: Анализ памяти

**Запрос**: "Сколько памяти используется?"

**Результат**:
```
Memory Status:
- Total: 3.82 GB
- Used: 0.86 GB
- Available: 2.97 GB
- Usage: 22.46%
- Status: NORMAL

Анализ LLM:
Использование памяти на уровне 22.46% является нормальным. 
Доступно 2.97 GB (77.5%) свободной памяти. Система имеет 
достаточный запас для пиковых нагрузок.
```

### Кейс 3: Поиск ошибок

**Запрос**: "Есть ли ошибки в логах?"

**Результат**:
```
Ошибок за последние 24ч не найдено ✓
```

## Выводы

### Достижения

✅ Создан рабочий прототип MCP-сервера  
✅ Реализованы 3 базовых tools с реальными данными  
✅ Интегрирована локальная LLM (Ollama)  
✅ Все компоненты протестированы и работают  
✅ Доказана концепция интеграции мониторинга + LLM

### Практическая ценность

- **Автоматизация анализа**: LLM анализирует метрики и дает рекомендации
- **Приватность**: Все данные остаются локально (Ollama)
- **Расширяемость**: Легко добавлять новые tools
- **Реальные данные**: Работа с production сервером

### Для диплома

Прототип демонстрирует:
1. Архитектуру интеграции LLM с мониторингом через MCP
2. Практическое применение больших языковых моделей
3. Гибридный подход: метрики + логи + AI анализ
4. Готовое решение для дальнейшего развития

---

**Дата последнего обновления**: 10 октября 2025  
**Статус**: ✅ WORKING PROTOTYPE - Ready for Demo

